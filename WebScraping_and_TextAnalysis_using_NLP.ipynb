{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehul6112/Data-Science_curve/blob/main/WebScraping_and_TextAnalysis_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "962018a2-2a86-4477-8ffe-892249e029b8",
      "metadata": {
        "id": "962018a2-2a86-4477-8ffe-892249e029b8"
      },
      "source": [
        "# Objective\n",
        "* The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ad9e29-df13-4ac6-99d5-adfdea7f5475",
      "metadata": {
        "id": "63ad9e29-df13-4ac6-99d5-adfdea7f5475"
      },
      "source": [
        "# Text Extraction\n",
        "## Approach\n",
        "\n",
        "### Part 1: Text Extraction\n",
        "\n",
        "**Fetch Article Content:**\n",
        "   - Read URLs from an Excel file.\n",
        "   - Extract article text and title from each URL using BeautifulSoup.\n",
        "   - Save each article's text into a file named with its `URL_ID`.\n",
        "\n",
        "\n",
        "\n",
        "## How to Run the .ipynb File to Generate Output\n",
        "\n",
        "### Ensure Required Libraries are Installed\n",
        "You can install them using pip:\n",
        "```bash\n",
        "pip install pandas openpyxl requests beautifulsoup4 nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470e61ce-c208-44a7-bfa5-b722e7bcbb7d",
      "metadata": {
        "id": "470e61ce-c208-44a7-bfa5-b722e7bcbb7d",
        "outputId": "86cd620f-961f-4b9d-a2b5-371aee9015c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fetching https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Error fetching https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Articles have been successfully extracted and saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "input_file = 'input.xlsx'\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "output_dir = 'articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for request errors\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Assuming the article title is within <h1> tags and the main content is within <article> tags\n",
        "        title = soup.find('h1').get_text(strip=True)\n",
        "        article_body = soup.find('article')\n",
        "\n",
        "        if not article_body:\n",
        "            # Fall back to a more general approach if <article> is not present\n",
        "            article_body = soup.find('div', class_='article-content') or soup.find('div', class_='post-content')\n",
        "\n",
        "        paragraphs = article_body.find_all('p')\n",
        "        article_text = '\\n'.join([para.get_text(strip=True) for para in paragraphs])\n",
        "\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Iterate through each URL and save the article content\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, content = extract_article_content(url)\n",
        "\n",
        "    if title and content:\n",
        "        output_file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(title + \"\\n\\n\" + content)\n",
        "\n",
        "print(\"Articles have been successfully extracted and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6eaf8a7-424a-49ca-947d-b1659eb4a038",
      "metadata": {
        "id": "e6eaf8a7-424a-49ca-947d-b1659eb4a038"
      },
      "source": [
        "### Looks like 2 articles raised error 404. Let's retry for these specific URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bede7448-535e-4828-b85f-4a846e6fdb2d",
      "metadata": {
        "id": "bede7448-535e-4828-b85f-4a846e6fdb2d",
        "outputId": "334899b4-61dc-40a0-dd7a-217a816e84e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrying extraction for 2 missing articles.\n",
            "Error fetching https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Failed to retrieve content for URL_ID blackassign0036\n",
            "Error fetching https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Failed to retrieve content for URL_ID blackassign0049\n",
            "Retry process completed.\n"
          ]
        }
      ],
      "source": [
        "missing_url_ids = []\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    output_file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
        "    if not os.path.isfile(output_file_path):\n",
        "        missing_url_ids.append(url_id)\n",
        "\n",
        "print(f\"Retrying extraction for {len(missing_url_ids)} missing articles.\")\n",
        "\n",
        "# Retry extraction for missing URL_IDs\n",
        "for url_id in missing_url_ids:\n",
        "    url = df.loc[df['URL_ID'] == url_id, 'URL'].values[0]\n",
        "    title, content = extract_article_content(url)\n",
        "\n",
        "    if title and content:\n",
        "        output_file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(title + \"\\n\\n\" + content)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content for URL_ID {url_id}\")\n",
        "\n",
        "\n",
        "print(\"Retry process completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce19f1a-b3cd-428c-8611-9153e09489ee",
      "metadata": {
        "id": "5ce19f1a-b3cd-428c-8611-9153e09489ee"
      },
      "source": [
        "### Out of 100 articles given in input.xlsx, url_id 36 and 49 seems to indicate that the requested URL is not available on the server. This error can occur if the URL has been removed or is incorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c332700-f429-4ad6-a520-646347a8d572",
      "metadata": {
        "id": "2c332700-f429-4ad6-a520-646347a8d572"
      },
      "source": [
        "# Text Analysis Instructions\n",
        "\n",
        "## Text Analysis Process\n",
        "\n",
        "For each file in the article directory, perform the following text analysis:\n",
        "\n",
        "1. **Sentiment Analysis:**\n",
        "   - Exclude words from the \"StopWords\" directory during sentiment analysis.\n",
        "   - Use the \"MasterDictionary\" directory containing \"positive-words.txt\" and \"negative-words.txt\".\n",
        "   - Convert the text into tokens using the NLTK tokenize module.\n",
        "   - Calculate the following variables:\n",
        "     - **Positive Score:** Assign +1 for each word found in the Positive Dictionary and sum all values.\n",
        "     - **Negative Score:** Assign -1 for each word found in the Negative Dictionary and sum all values (multiplied by -1 to ensure positive score).\n",
        "     - **Polarity Score:** Calculate as (Positive Score – Negative Score)/((Positive Score + Negative Score) + 0.000001). Range is from -1 to +1.\n",
        "     - **Subjectivity Score:** Calculate as (Positive Score + Negative Score)/((Total Words after cleaning) + 0.000001). Range is from 0 to +1.\n",
        "\n",
        "2. **Readability Analysis:**\n",
        "   - Use the Gunning Fox index formula:\n",
        "     - **Average Sentence Length:** Calculate as the number of words / the number of sentences.\n",
        "     - **Percentage of Complex Words:** Calculate as the number of complex words / the number of words.\n",
        "     - **Fog Index:** Calculate as 0.4 * (Average Sentence Length + Percentage of Complex words).\n",
        "\n",
        "3. **Additional Metrics:**\n",
        "   - **Average Number of Words Per Sentence:** Calculate as the total number of words / the total number of sentences.\n",
        "   - **Complex Word Count:** Count words in the text containing more than two syllables.\n",
        "   - **Word Count:** Count the total cleaned words present in the text by removing stop words (using NLTK stopwords) and punctuations like ? ! , . from each word before counting.\n",
        "   - **Syllable Count Per Word:** Count the number of syllables in each word of the text by counting the vowels present in each word, handling exceptions like words ending with \"es\" or \"ed\".\n",
        "   - **Personal Pronouns:** Use regex to find counts of words such as “I,” “we,” “my,” “ours,” and “us”, excluding the country name \"US\".\n",
        "   - **Average Word Length:** Calculate the average length of words.\n",
        "   \n",
        "4. **Tokenization and Syllable Counting:**\n",
        "   - Use `nltk` library to tokenize the text into words and sentences.\n",
        "   - Count syllables in each word.\n",
        "\n",
        "5. **Merge and Save Results:**\n",
        "   - Merge the original DataFrame with the results DataFrame on the `URL_ID` column.\n",
        "   - Save the combined DataFrame to an Excel file named `output.xlsx`.\n",
        "\n",
        "## How to Run the Text Analysis\n",
        "\n",
        "1. **Ensure Required Libraries are Installed:**\n",
        "   Ensure you have the required libraries installed. You can install them using pip:\n",
        "   ```bash\n",
        "   pip install pandas nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73dd47e-9700-466c-9bdf-3b924ec02629",
      "metadata": {
        "id": "e73dd47e-9700-466c-9bdf-3b924ec02629",
        "outputId": "2ab00c66-7839-445f-edc2-9868422835b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis and concatenation completed. Results saved to output.xlsx.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_dir = 'StopWords'\n",
        "master_dict_dir = 'MasterDictionary'\n",
        "\n",
        "# Load positive and negative words\n",
        "with open(os.path.join(master_dict_dir, 'positive-words.txt'), 'r') as f:\n",
        "    positive_words = set(f.read().split())\n",
        "with open(os.path.join(master_dict_dir, 'negative-words.txt'), 'r') as f:\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Load all stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for file in os.listdir(stopwords_dir):\n",
        "    with open(os.path.join(stopwords_dir, file), 'r') as f:\n",
        "        stop_words.update(f.read().split())\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_tokenize(text):\n",
        "    # Remove punctuation and tokenize\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Function to calculate sentiment scores\n",
        "def sentiment_analysis(tokens):\n",
        "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
        "    negative_score = sum(-1 for word in tokens if word in negative_words) * -1\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Function to calculate readability scores\n",
        "def readability_analysis(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    avg_sentence_length = len(words) / len(sentences)\n",
        "    complex_words_count = sum(1 for word in words if count_syllables(word) > 2)\n",
        "    percent_complex_words = complex_words_count / len(words)\n",
        "    fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
        "    avg_words_per_sentence = len(words) / len(sentences)\n",
        "    return avg_sentence_length, percent_complex_words, fog_index, avg_words_per_sentence, complex_words_count\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    syllable_count = len(re.findall(r'[aeiouy]+', word))\n",
        "    if word.endswith('es') or word.endswith('ed'):\n",
        "        syllable_count = max(1, syllable_count - 1)\n",
        "    return syllable_count\n",
        "\n",
        "# Function to count personal pronouns\n",
        "def count_personal_pronouns(text):\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
        "    return len(pronouns)\n",
        "\n",
        "# Function to analyze each file in the articles directory\n",
        "def analyze_files(directory):\n",
        "    results = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "            tokens = clean_tokenize(text)\n",
        "            if len(tokens) < 50:\n",
        "                continue\n",
        "\n",
        "            positive_score, negative_score, polarity_score, subjectivity_score = sentiment_analysis(tokens)\n",
        "            avg_sentence_length, percent_complex_words, fog_index, avg_words_per_sentence, complex_words_count = readability_analysis(text)\n",
        "            total_words = len(tokens)\n",
        "            syllable_counts = [count_syllables(word) for word in tokens]\n",
        "            avg_syllables_per_word = sum(syllable_counts) / total_words\n",
        "            personal_pronouns_count = count_personal_pronouns(text)\n",
        "            avg_word_length = sum(len(word) for word in tokens) / total_words\n",
        "\n",
        "            results.append({\n",
        "                'URL_ID': file_name.replace('.txt', ''),\n",
        "                'Positive Score': positive_score,\n",
        "                'Negative Score': negative_score,\n",
        "                'Polarity Score': polarity_score,\n",
        "                'Subjectivity Score': subjectivity_score,\n",
        "                'Average Sentence Length': avg_sentence_length,\n",
        "                'Percentage of Complex Words': percent_complex_words,\n",
        "                'Fog Index': fog_index,\n",
        "                'Average Words Per Sentence': avg_words_per_sentence,\n",
        "                'Complex Word Count': complex_words_count,\n",
        "                'Word Count': total_words,\n",
        "                'Syllable Count Per Word': avg_syllables_per_word,\n",
        "                'Personal Pronouns Count': personal_pronouns_count,\n",
        "                'Average Word Length': avg_word_length\n",
        "            })\n",
        "    return results\n",
        "\n",
        "# Directory where the articles are saved\n",
        "output_dir = 'articles'\n",
        "\n",
        "# Perform analysis on each file\n",
        "results = analyze_files(output_dir)\n",
        "\n",
        "# Convert results to DataFrame for tabular output\n",
        "results_df = pd.DataFrame(results)\n",
        "combined_df = df.merge(results_df, on='URL_ID')\n",
        "\n",
        "# Write the combined DataFrame to an Excel file\n",
        "output_file = 'output.xlsx'\n",
        "combined_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Analysis and concatenation completed. Results saved to output.xlsx.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82868b51-6136-4a38-a829-1f94c21f0e99",
      "metadata": {
        "id": "82868b51-6136-4a38-a829-1f94c21f0e99"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac65a6b-8a3d-4fef-9f6d-b1a1620b6150",
      "metadata": {
        "id": "cac65a6b-8a3d-4fef-9f6d-b1a1620b6150",
        "outputId": "2aa4f4a5-bdbd-4872-be9c-995085568ea4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "      <th>Positive Score</th>\n",
              "      <th>Negative Score</th>\n",
              "      <th>Polarity Score</th>\n",
              "      <th>Subjectivity Score</th>\n",
              "      <th>Average Sentence Length</th>\n",
              "      <th>Percentage of Complex Words</th>\n",
              "      <th>Fog Index</th>\n",
              "      <th>Average Words Per Sentence</th>\n",
              "      <th>Complex Word Count</th>\n",
              "      <th>Word Count</th>\n",
              "      <th>Syllable Count Per Word</th>\n",
              "      <th>Personal Pronouns Count</th>\n",
              "      <th>Average Word Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>blackassign0001</td>\n",
              "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.042169</td>\n",
              "      <td>15.760000</td>\n",
              "      <td>0.126904</td>\n",
              "      <td>6.354761</td>\n",
              "      <td>15.760000</td>\n",
              "      <td>50</td>\n",
              "      <td>166</td>\n",
              "      <td>2.168675</td>\n",
              "      <td>4</td>\n",
              "      <td>6.487952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blackassign0002</td>\n",
              "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
              "      <td>54</td>\n",
              "      <td>29</td>\n",
              "      <td>0.301205</td>\n",
              "      <td>0.109499</td>\n",
              "      <td>21.440000</td>\n",
              "      <td>0.212687</td>\n",
              "      <td>8.661075</td>\n",
              "      <td>21.440000</td>\n",
              "      <td>342</td>\n",
              "      <td>758</td>\n",
              "      <td>2.490765</td>\n",
              "      <td>6</td>\n",
              "      <td>7.544855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>blackassign0003</td>\n",
              "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
              "      <td>38</td>\n",
              "      <td>24</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>21.803571</td>\n",
              "      <td>0.292383</td>\n",
              "      <td>8.838382</td>\n",
              "      <td>21.803571</td>\n",
              "      <td>357</td>\n",
              "      <td>620</td>\n",
              "      <td>2.824194</td>\n",
              "      <td>13</td>\n",
              "      <td>8.293548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>blackassign0004</td>\n",
              "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
              "      <td>37</td>\n",
              "      <td>75</td>\n",
              "      <td>-0.339286</td>\n",
              "      <td>0.184211</td>\n",
              "      <td>23.745098</td>\n",
              "      <td>0.270025</td>\n",
              "      <td>9.606049</td>\n",
              "      <td>23.745098</td>\n",
              "      <td>327</td>\n",
              "      <td>608</td>\n",
              "      <td>2.720395</td>\n",
              "      <td>5</td>\n",
              "      <td>8.080592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>blackassign0005</td>\n",
              "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
              "      <td>20</td>\n",
              "      <td>8</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.076294</td>\n",
              "      <td>19.589744</td>\n",
              "      <td>0.201571</td>\n",
              "      <td>7.916526</td>\n",
              "      <td>19.589744</td>\n",
              "      <td>154</td>\n",
              "      <td>367</td>\n",
              "      <td>2.381471</td>\n",
              "      <td>6</td>\n",
              "      <td>7.534060</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            URL_ID                                                URL  \\\n",
              "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
              "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
              "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
              "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
              "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
              "\n",
              "   Positive Score  Negative Score  Polarity Score  Subjectivity Score  \\\n",
              "0               6               1        0.714286            0.042169   \n",
              "1              54              29        0.301205            0.109499   \n",
              "2              38              24        0.225806            0.100000   \n",
              "3              37              75       -0.339286            0.184211   \n",
              "4              20               8        0.428571            0.076294   \n",
              "\n",
              "   Average Sentence Length  Percentage of Complex Words  Fog Index  \\\n",
              "0                15.760000                     0.126904   6.354761   \n",
              "1                21.440000                     0.212687   8.661075   \n",
              "2                21.803571                     0.292383   8.838382   \n",
              "3                23.745098                     0.270025   9.606049   \n",
              "4                19.589744                     0.201571   7.916526   \n",
              "\n",
              "   Average Words Per Sentence  Complex Word Count  Word Count  \\\n",
              "0                   15.760000                  50         166   \n",
              "1                   21.440000                 342         758   \n",
              "2                   21.803571                 357         620   \n",
              "3                   23.745098                 327         608   \n",
              "4                   19.589744                 154         367   \n",
              "\n",
              "   Syllable Count Per Word  Personal Pronouns Count  Average Word Length  \n",
              "0                 2.168675                        4             6.487952  \n",
              "1                 2.490765                        6             7.544855  \n",
              "2                 2.824194                       13             8.293548  \n",
              "3                 2.720395                        5             8.080592  \n",
              "4                 2.381471                        6             7.534060  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f9ca6c-88ff-4f79-b149-b6230f18db31",
      "metadata": {
        "id": "42f9ca6c-88ff-4f79-b149-b6230f18db31"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}